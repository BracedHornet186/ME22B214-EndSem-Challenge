# Mixture of Experts for Metric Score Prediction
Author: Yash Purswani \
Roll No: ME22B214

This repository contains the code for predicting metric learning scores using a **Mixture-of-Experts (MoE)** model built on top of **SBERT text embeddings** and **metric embeddings**.  
The core workflow lives in the Jupyter notebook:

- `ME22B214.ipynb`

The notebook loads the raw JSON data, builds dense embeddings, performs feature engineering, trains a mixture-of-experts model, calibrates it, and finally exports a Kaggle-style submission file.

---

## Project Structure

```text
.
├── ME22B214.ipynb          # Main notebook (end-to-end pipeline)
├── data/
│   ├── train_data.json     # Training samples with prompts, responses, metric_name, score
│   ├── test_data.json      # Test samples without score
│   ├── metric_name_embeddings.npy  # Precomputed embeddings for each metric_name
│   ├── metric_names.json   # List of metric names (index aligned with embeddings)
│   ├── y_train_original.npy    # Original training scores (0–10)
│   ├── X_train_augmented.npy   # Saved training features (generated by notebook)
│   ├── y_train_augmented.npy   # Saved training labels (generated by notebook)
│   ├── X_test.npy              # Baseline test features (if provided)
│   └── X_test_augmented.npy    # Final test features (generated by notebook)
└── submission_me22b214.csv # Final prediction file (generated)
```

# Method Overview

## 1. Text & Metric Embeddings

### Text Embedding Process
- **Join text fields** into one sequence:
  ```
  system_prompt [SEP] user_prompt [SEP] response
  ```

- **Sentence-BERT model** used:
  - Model: `l3cube-pune/indic-sentence-similarity-sbert`
  - Computes a text embedding for each sample

### Metric Embedding
- Looks up corresponding metric embedding from `metric_name_embeddings.npy`
- Uses `metric_names.json` for mapping
- **Concatenates** text and metric embeddings to form the base feature vector

### Output Files
The notebook builds:
- `X_train_augmented.npy`, `y_train_augmented.npy`
- `X_test_augmented.npy`

All train/test samples are encoded using this process.


## 2. Data Augmentation (Score-Level)

### Augmentation Strategy
- Randomly copies training samples
- Assigns them a **low score (0)** for a randomly chosen `metric_name`
- **Purpose**: Increases representation of low-score examples
- **Benefit**: Helps the model learn better in the low-score region

The combined augmented list `train_aug` is then embedded using the process above.


## 3. Feature Engineering

### Distance & Similarity Features
From the split text embeddings and metric embeddings, computes:
- Cosine similarity
- Dot product
- L1 distance
- L2 distance
- Norms of text embeddings
- Norms of metric embeddings

### PCA-Based Interaction Features
Applies dimensionality reduction to capture interactions:
- **KernelPCA / PCA** applied to:
  - Text embeddings
  - Metric embeddings
  - Interaction features:
    - Sum of embeddings
    - Product of embeddings
    - Absolute value operations
    - Other combinations

### Output
Stacks all features to form:
- Enhanced feature matrix **`X_feat`** (training)
- Enhanced feature matrix **`X_feat_test`** (test data)


## 4. Mixture-of-Experts (MoE) Model

### Score Range Discretization
The continuous score range **[0, 10]** is split into three ordinal classes:
- **Low**: 0–3
- **Mid**: 4–7
- **High**: 8–10

### Step 1: Classifier (Expert Gating)
- **Model**: LightGBM Classifier (`LGBMClassifier`)
- **Objective**: `multiclass` with 3 classes
- **Training**: Uses engineered features `X_feat`
- **Output**: Predicts class probabilities for each sample:
  - `P_low`
  - `P_mid`
  - `P_high`

### Step 2: Per-Region Regressors (Experts)
Three specialized LightGBM regressors are trained:
- **`reg_low`**: Trained on low-score samples (0–3)
- **`reg_mid`**: Trained on mid-score samples (4–7)
- **`reg_high`**: Trained on high-score samples (8–10)

Each regressor specializes in predicting scores within its designated region.

### Step 3: Mixture Prediction
For each sample:
1. Compute expert predictions:
   - `pred_low` from `reg_low`
   - `pred_mid` from `reg_mid`
   - `pred_high` from `reg_high`

2. Combine predictions using soft class probabilities:
   ```
   final_score = P_low × pred_low + P_mid × pred_mid + P_high × pred_high
   ```

This weighted combination allows the model to leverage specialized experts while smoothly transitioning between score regions.


## Summary

This pipeline combines:
- **Semantic embeddings** (text + metric)
- **Feature engineering** (distances, similarities, PCA interactions)
- **Mixture-of-Experts architecture** (classification-guided regression)

The MoE approach ensures that different score ranges are handled by specialized models, improving overall prediction accuracy across the entire score distribution.